{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import xlsxwriter as xl\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "import scipy.io as sio\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import string\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmas=WordNetLemmatizer()\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1g = [] \n",
    "numSentences = 243\n",
    "with open('textfile.txt', 'r') as file:\n",
    "    for j in range(numSentences):\n",
    "        line = file.readline().strip('\\n')\n",
    "        line = line.split(' ')\n",
    "\n",
    "        f1g.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('textfile.txt', 'r') as file1:\n",
    "    f1 = file1.readlines()\n",
    "    \n",
    "f1e = []\n",
    "f1e2 = []\n",
    "for sentence in f1:\n",
    "    sentence = sentence.split(' ')\n",
    "    f1e.append(sentence)\n",
    "    for word in sentence:\n",
    "        word = word.replace('\\n','')\n",
    "        f1e2.append(word)\n",
    "        \n",
    "print('No. of words: ' + str(len(f1e2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgWordSent = len(f1e2)/numSentences\n",
    "print(avgWordSent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglst = []\n",
    "for sentence in f1g:\n",
    "    tag = pos_tag(sentence)\n",
    "    taglst.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('POS tags for each word:')\n",
    "print(taglst[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbering of sentences (and division into passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snlst = [] \n",
    "ncounter = 1\n",
    "for sentence in f1g:\n",
    "    for word in sentence:\n",
    "        snlst.append(ncounter)\n",
    "\n",
    "        for char in word:\n",
    "            if char != '.':\n",
    "                pass\n",
    "            elif char == '.':\n",
    "                ncounter += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passage labeling (only if label file provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsPassages = sio.loadmat('MATLABsentencelabels')\n",
    "\n",
    "lP = labelsPassages['sentencelabels']\n",
    "lP = np.hsplit(lP,1)\n",
    "\n",
    "lplst = np.array(lP).tolist()\n",
    "lplst = lplst[0][0]\n",
    "\n",
    "# Appending the passage number to words\n",
    "lplst_word = []\n",
    "counter = 0\n",
    "for sentence in f1g:\n",
    "    if counter == numSentences+2:\n",
    "        break\n",
    "    for word in sentence:\n",
    "        lplst_word.append(lplst[counter])\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence number within passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlst = []\n",
    "for sentence in f1g:\n",
    "    for word in sentence:\n",
    "        wordlst.append(word)\n",
    "\n",
    "print('List containing separate words: '+ str(wordlst))\n",
    "\n",
    "word_lp_tup = list(zip(wordlst,lplst_word))\n",
    "\n",
    "seen = set()\n",
    "\n",
    "counter = 1\n",
    "snplst = [] # sentence number within passage list\n",
    "for tup in word_lp_tup:\n",
    "    first = tup[0] # first index of the tuple, i.e. the word\n",
    "    second = tup[1]\n",
    "\n",
    "    if second not in seen:\n",
    "        counter = 1 # restarting the counter if the passage number changes\n",
    "        \n",
    "    seen.add(second)\n",
    "    snplst.append(counter)\n",
    "\n",
    "    for char in first:\n",
    "        if char != '.':\n",
    "            continue\n",
    "        if char == '.':\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word number within sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_no = 1\n",
    "wnslst = [] # word number sentence list\n",
    "for word in wordlst:\n",
    "    \n",
    "    wnslst.append(counter_no)\n",
    "    counter_no += 1\n",
    "    for char in word:\n",
    "        if char != '.':\n",
    "            continue\n",
    "        if char == '.':\n",
    "            counter_no = 1\n",
    "\n",
    "print('Word numbering within sentence: '+str(wnslst[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad topic for each sentence (if topics provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyPassageCategory = ['astronaut','beekeeping','blindness','bone_fracture','castle','computer_graphics','dreams','gambling','hurricane','ice_cream','infection','law_school','lawn_mower','opera','owl','painter','pharmacist','polar_bear','pyramid','rock_climbing','skiing','stress','taste','tuxedo']\n",
    "\n",
    "countlst = list(range(1,73))\n",
    "\n",
    "# Making a tuple of categories and numbering\n",
    "\n",
    "pascat = list(zip(countlst,keyPassageCategory))\n",
    "\n",
    "print('Numbering of topics: ' + str(pascat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsPassageCategory = sio.loadmat('MATLABlabelsPassage')\n",
    "\n",
    "lPC = labelsPassageCategory['labelsPassage']\n",
    "lPC = np.hsplit(lPC,1)\n",
    "\n",
    "lpclst = np.array(lPC).tolist()\n",
    "lpclst = lpclst[0]\n",
    "\n",
    "lpclst = list(chain.from_iterable(lpclst)) # Accessing the nested lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpclst_word = []\n",
    "\n",
    "for value in lplst_word: # Iterating through the passage number list\n",
    "    lpclst_word.append(lpclst[value-1]) # Appending values from the lpclst, which contains category labels\n",
    "    \n",
    "# Need to match the category label list: lpclst with the actual category names, keyPassageCategory\n",
    "\n",
    "catlst = []\n",
    "\n",
    "for value in lpclst_word:\n",
    "    for tup in pascat:\n",
    "        first = tup[0]\n",
    "        second = tup[1]\n",
    "        \n",
    "        if value == first:\n",
    "            catlst.append(second)\n",
    "\n",
    "print('Category labels for each word: ' + str(catlst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all unique characters (letters, numbers and punctuation) in the corpus\n",
    "\n",
    "# Creating a character list\n",
    "\n",
    "charlst = []\n",
    "for word in wordlst:\n",
    "    for char in word:\n",
    "        charlst.append(char)\n",
    "\n",
    "uniquechar = set(charlst)\n",
    "print('Unique characters: ' + str(uniquechar))\n",
    "\n",
    "nonletters = ['-','.',\"'\",'7',',','8','4',':','0','$','1','9']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripping punctuation and capitalized letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping wordlist for punctuation\n",
    "\n",
    "wordlst_np = wordlst # wordlist no punctuation\n",
    "\n",
    "wordlst_np = [\"\".join( j for j in i if j not in string.punctuation) for i in wordlst_np]     \n",
    "\n",
    "print('Word list with no punctuation: ' + str(wordlst_np)) #Stripped list!\n",
    "\n",
    "charcount = 0\n",
    "wordlen = []\n",
    "for word in wordlst_np:\n",
    "    for char in word:\n",
    "        charcount += 1\n",
    "    wordlen.append(charcount)\n",
    "    charcount = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing capitalized letters\n",
    "\n",
    "wordlst_l = [] # wordlist lowercase\n",
    "for word in wordlst_np:\n",
    "    lower_word = word.lower()\n",
    "    wordlst_l.append(lower_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing capitalized letters from original list, thus with punctuation\n",
    "wordlst_lower = [] # wordlist lowercase\n",
    "for word in wordlst:\n",
    "    lower_word = word.lower()\n",
    "    wordlst_lower.append(lower_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all the different POS tags in taglst\n",
    "\n",
    "unique_tags = []\n",
    "for sent in taglst:\n",
    "    for tup in sent:\n",
    "        second = tup[1]\n",
    "        unique_tags.append(second)\n",
    "\n",
    "unique_tags = set(unique_tags)\n",
    "print('Unique POS tags: ' + str(sorted(unique_tags)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of chosen POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only POS tags and zip it together with the wordlst_l\n",
    "\n",
    "POS_tags=[]\n",
    "# Make tags lowercase\n",
    "for entry in taglst:\n",
    "    for tup in entry:\n",
    "        tag=tup[1]\n",
    "        #tag = tag.lower()\n",
    "        POS_tags.append(tag)\n",
    "\n",
    "# make all noun tags to 'n' and all verb tags to 'v' for the lemmatizer to work\n",
    "POS_nouns=['NN','NNS','NNP']\n",
    "POS_verbs=['VBD','VBG','VBN','VBP','VBZ'] \n",
    "POS_tags_s=[] #POS tags short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nouns\n",
    "for tag in POS_tags:\n",
    "    tagfound_N=[i for i, j in enumerate(POS_nouns) if j == tag]\n",
    "\n",
    "    if len(tagfound_N)>0:\n",
    "        POS_tags_s.append('n')        \n",
    "    if len(tagfound_N) == 0:\n",
    "        POS_tags_s.append(tag)\n",
    "\n",
    "# for verbs\n",
    "POS_tags2=[]\n",
    "for tag in POS_tags_s:\n",
    "    tagfound_V=[k for k, g in enumerate(POS_verbs) if g == tag]\n",
    "    if len(tagfound_V)>0:\n",
    "        POS_tags2.append('v')        \n",
    "    if len(tagfound_V) == 0:\n",
    "        POS_tags2.append(tag)\n",
    "        \n",
    "taglst_l = list(zip(wordlst_l,POS_tags2))       \n",
    "print('POS tags lowercase letters: ' + str(taglst_l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all nouns and verbs to lemmatized vwords\n",
    "wordlst_lem=[] # name of the list with lemmatized words\n",
    "\n",
    "for counter,tag in enumerate(taglst_l):\n",
    "    if tag[1] == 'n':\n",
    "        lem=lemmas.lemmatize(tag[0],'n')\n",
    "        wordlst_lem.append(lem)\n",
    "    if tag[1] == 'v':\n",
    "        lem=lemmas.lemmatize(tag[0],'v')\n",
    "        wordlst_lem.append(lem)\n",
    "    if tag[1] != 'n' and tag[1]!='v':\n",
    "        wordlst_lem.append(tag[0])\n",
    "        \n",
    "print('POS tags with lemmatized nouns and verbs: ' + str(wordlst_lem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare entries in lemmatized list and non-lemmatized list\n",
    "\n",
    "diff_lem=[i for i, j in zip(wordlst_l, wordlst_lem) if i != j]\n",
    "\n",
    "print('Number of edited entries by lemmatization: ' + str(len(diff_lem))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Count NA entries - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting NA values in tuple\n",
    "\n",
    "def countTupNA(tuplelist):\n",
    "    '''Count the number of NA values in a tuple consisting of word and value'''\n",
    "    NAcounter = 0\n",
    "    for tup in tuplelist:\n",
    "        second = tup[1]\n",
    "        if second.count('#NA') == 1:\n",
    "            NAcounter +=1\n",
    "    \n",
    "    return NAcounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting NA values in list\n",
    "\n",
    "def countNA(lst):\n",
    "    '''Count the number of NA values in a word list'''\n",
    "    NAcounter = 0\n",
    "    for value in lst:\n",
    "        if value.count('#NA') == 1:\n",
    "            NAcounter +=1\n",
    "    \n",
    "    return NAcounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing word features: Age of acquisition, valence, arousal, prevalence, lexical frequency, concreteness, possible POS tags, ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(wordlist,feature):\n",
    "    '''Function for acquiring feature values. Requires download of the database files (see readme file).\n",
    "    \n",
    "    Input: 1) list of words. 2) feature: AoA, valence, arousal, prevalence,\n",
    "    lexical frequency, concreteness, possible POS tags, ambiguity\n",
    "\n",
    "    Return: 1) list of solely feature values\n",
    "            2) tuple corresponding of the word and feature value'''\n",
    "    firstVal = []\n",
    "    secondVal = []\n",
    "    \n",
    "    # AoA EXCEL FILE #\n",
    "    if feature == 'aoa':\n",
    "        excelFile = pd.read_excel('AoA.xlsx')\n",
    "        formatExcel = ['Word', 'Rating.Mean']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "\n",
    "        for index, row in excelFile.iterrows(): #Iterating over rows, i.e. words\n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['Rating.Mean']\n",
    "            secondVal.append(second)\n",
    "    \n",
    "    # Valence EXCEL FILE #\n",
    "    if feature == 'valence':\n",
    "        excelFile = pd.read_excel('valence.xlsx')\n",
    "        formatExcel = ['Word', 'V.Mean.Sum']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['V.Mean.Sum']\n",
    "            secondVal.append(second)\n",
    "    \n",
    "    # Arousal EXCEL FILE #\n",
    "    if feature == 'arousal':\n",
    "        excelFile = pd.read_excel('valence.xlsx')\n",
    "        formatExcel = ['Word', 'A.Mean.Sum']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['A.Mean.Sum']\n",
    "            secondVal.append(second)\n",
    "            \n",
    "    # Prevalence EXCEL FILE #\n",
    "    if feature == 'prevalence':\n",
    "        excelFile = pd.read_excel('prevalence.xlsx')\n",
    "        formatExcel = ['Word', 'Prevalence']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['Prevalence']\n",
    "            secondVal.append(second)\n",
    "            \n",
    "    # Lexical frequency EXCEL FILE #\n",
    "    if feature == 'frequency':\n",
    "        excelFile = pd.read_excel('wordmatch.xlsx')\n",
    "        formatExcel = ['Word', 'Lexical_frequency']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['Lexical_frequency']\n",
    "            secondVal.append(second)\n",
    "            \n",
    "    # Concreteness EXCEL FILE #\n",
    "    if feature == 'concreteness':\n",
    "        excelFile = pd.read_excel('concreteness.xlsx')\n",
    "        formatExcel = ['Word', 'Conc.M']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['Conc.M']\n",
    "            secondVal.append(second)\n",
    "    \n",
    "    # Possible EXCEL FILE (possible tags) #\n",
    "    if feature == 'possible':\n",
    "        excelFile = pd.read_excel('wordmatch.xlsx')\n",
    "        formatExcel = ['Word', 'All_PoS_SUBTLEX']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['All_PoS_SUBTLEX']\n",
    "            secondVal.append(second)\n",
    "            \n",
    "    # Ambiguity EXCEL FILE #\n",
    "    if feature == 'ambiguity':\n",
    "        excelFile = pd.read_excel('wordmatch.xlsx')\n",
    "        formatExcel = ['Word', 'Percentage_dom_PoS']\n",
    "        wordVal = excelFile[formatExcel]\n",
    "        \n",
    "        for index, row in excelFile.iterrows(): \n",
    "            first = row['Word']\n",
    "            firstVal.append(first)\n",
    "            second = row['Percentage_dom_PoS']\n",
    "            secondVal.append(second)\n",
    "    \n",
    "    wordvals_feature = []\n",
    "    wordlst_feature = []\n",
    "    for word in wordlist:\n",
    "        wordfound_feature=[i for i, j in enumerate(firstVal) if j == word]\n",
    "        wordval_feature=[secondVal[i] for i, j in enumerate(firstVal) if j == word]\n",
    "\n",
    "        if len(wordfound_feature) > 0:\n",
    "            wordvals_feature.append(wordval_feature)\n",
    "            wordlst_feature.append(word)\n",
    "\n",
    "        if len(wordfound_feature) == 0:\n",
    "            wordvals_feature.append(['#NA'])\n",
    "            wordlst_feature.append(word)\n",
    "\n",
    "    final_feature = list(zip(wordlst_feature,wordvals_feature))\n",
    "    \n",
    "    return wordvals_feature, final_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content/function word tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content: nouns, verbs, adjectives and adverbs. 1\n",
    "# Function: the rest. 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different POS tags \n",
    "\n",
    "seenPOS=set()\n",
    "first_cf = []\n",
    "second_cf = []\n",
    "\n",
    "for sentence in taglst:\n",
    "    for tup in sentence:\n",
    "        first = tup[0]\n",
    "        first_cf.append(first)\n",
    "    \n",
    "        second = tup[1]\n",
    "        seenPOS.add(second)\n",
    "        second_cf.append(second)\n",
    "\n",
    "content_lst = ['VB','NN','NNP','NNS','JJ','JJS','JJR','VBD','VBG','VBN','VBP','VBZ','RB','RBR','RBS','WRB']\n",
    "print(sorted(seenPOS))\n",
    "print(sorted(content_lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_lst = []\n",
    "\n",
    "for tag in second_cf:\n",
    "    \n",
    "    tagfound=[i for i, j in enumerate(content_lst) if j == tag]\n",
    "    \n",
    "    if len(tagfound) == 1:\n",
    "        binary_lst.append(1)\n",
    "    \n",
    "    \n",
    "    if len(tagfound) == 0:\n",
    "        binary_lst.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Content/function word list: ' + str(len(binary_lst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical surprisal - non lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colala.bcs.rochester.edu/data/PiantadosiTilyGibson2011/Google10L-1T/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four different files for lexical surprisal. \n",
    "\n",
    "file1=open('surprisal-1.txt','r')\n",
    "text1=file1.read()\n",
    "#print(text)\n",
    "file1.close()\n",
    "\n",
    "textspl1=text1.split()\n",
    "\n",
    "file2=open('surprisal-2.txt','r')\n",
    "text2=file2.read()\n",
    "#print(text)\n",
    "file2.close()\n",
    "\n",
    "textspl2=text2.split()\n",
    "\n",
    "file3=open('surprisal-3.txt','r')\n",
    "text3=file3.read()\n",
    "file3.close()\n",
    "\n",
    "textspl3=text3.split()\n",
    "\n",
    "file4=open('surprisal-4.txt','r')\n",
    "text4=file4.read()\n",
    "\n",
    "textspl4=text4.split()\n",
    "file4.close()\n",
    "\n",
    "merged_lst = textspl1+textspl2+textspl3+textspl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpVals=[]\n",
    "\n",
    "for w in wordlst_l:\n",
    "\n",
    "    str1='\"'+w+'\"';\n",
    "    \n",
    "    try:\n",
    "        fw=merged_lst.index(str1)\n",
    "    except:\n",
    "        surpVals.append('[#NA]')\n",
    "        continue\n",
    "\n",
    "    surp_val=merged_lst[fw+3]\n",
    "\n",
    "    surpVals.append(surp_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Surprisal values for word list: ' + str(surpVals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze lexical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoaVals, aoaTup = getFeature(wordlst_l,'aoa')\n",
    "aoaVals_lem, aoaTup_lem = getFeature(wordlst_lem,'aoa')\n",
    "\n",
    "valVals, valTup = getFeature(wordlst_l,'valence')\n",
    "valVals_lem, valTup_lem = getFeature(wordlst_lem,'valence')\n",
    "\n",
    "arVals, arTup = getFeature(wordlst_l,'arousal')\n",
    "arVals_lem, arTup_lem = getFeature(wordlst_lem,'arousal')\n",
    "\n",
    "prevVals, prevTup = getFeature(wordlst_l,'prevalence')\n",
    "prevVals_lem, prevTup_lem = getFeature(wordlst_lem,'prevalence')\n",
    "\n",
    "freqVals, freqTup = getFeature(wordlst_l,'frequency')\n",
    "freqVals_lem, freqTup_lem = getFeature(wordlst_lem,'frequency')\n",
    "\n",
    "concVals, concTup = getFeature(wordlst_l,'concreteness')\n",
    "concVals_lem, concTup_lem = getFeature(wordlst_lem,'concreteness')\n",
    "\n",
    "possVals, possTup = getFeature(wordlst_l,'possible')\n",
    "possVals_lem, possTup_lem = getFeature(wordlst_lem,'possible')\n",
    "\n",
    "ambVals, ambTup = getFeature(wordlst_l,'ambiguity')\n",
    "ambVals_lem, ambTup_lem = getFeature(wordlst_lem,'ambiguity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge lemmatized and non-lemmatized word lists - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeList(nonLemVals,lemVals):\n",
    "    '''Input: Two lists.\n",
    "       Return: If list 1 contains NA vals, the NA val is replaced by the value in list 2 (either numerical value or #NA again)\n",
    "    '''\n",
    "\n",
    "    mergeCount = 0\n",
    "    mergeLst = []\n",
    "    NAcounter = 0\n",
    "    for value in nonLemVals:    \n",
    "        if value != ['#NA']:\n",
    "            mergeLst.append(value)\n",
    "\n",
    "        if value == ['#NA']:\n",
    "            lemVal = lemVals[mergeCount]\n",
    "            mergeLst.append(lemVal)\n",
    "            NAcounter += 1\n",
    "        mergeCount += 1\n",
    "        \n",
    "    return mergeLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all non-lemmatized and lemmatized lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoaMerge = mergeList(aoaVals,aoaVals_lem)\n",
    "\n",
    "valMerge = mergeList(valVals,valVals_lem)\n",
    "\n",
    "arMerge = mergeList(arVals,arVals_lem)\n",
    "\n",
    "prevMerge = mergeList(prevVals,prevVals_lem)\n",
    "\n",
    "freqMerge = mergeList(freqVals,freqVals_lem)\n",
    "\n",
    "concMerge = mergeList(concVals,concVals_lem)\n",
    "\n",
    "possMerge = mergeList(possVals,possVals_lem)\n",
    "\n",
    "ambMerge = mergeList(ambVals,ambVals_lem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel appending - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabetNum = list(range(0,27))\n",
    "letterCount = dict(zip(string.ascii_uppercase, alphabetNum))\n",
    "\n",
    "def writeCol(colnumber,colheader,appendlist):\n",
    "    sheet1.write(colnumber,colheader)\n",
    "    add_counter = 0  \n",
    "    \n",
    "    numInput = colnumber[:-1]\n",
    "    numAppend = letterCount.get(numInput) # Which column to append to\n",
    "    \n",
    "    for number in appendlist:\n",
    "        add_counter += 1\n",
    "        sheet1.write(add_counter,numAppend,number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabetNum = list(range(0,27))\n",
    "letterCount = dict(zip(string.ascii_uppercase, alphabetNum))\n",
    "\n",
    "def writeColList(colnumber,colheader,appendlist):\n",
    "    '''For list-type items, i.e. if lists in lists'''\n",
    "    sheet1.write(colnumber,colheader)\n",
    "    add_counter = 0  \n",
    "    \n",
    "    numInput = colnumber[:-1]\n",
    "    numAppend = letterCount.get(numInput) # Which column to append to\n",
    "    \n",
    "    for number in appendlist:\n",
    "        newNum = number[0]\n",
    "        add_counter += 1\n",
    "        sheet1.write(add_counter,numAppend,newNum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = xl.Workbook('name.xlsx')\n",
    "sheet1 = wb.add_worksheet('sheet1')\n",
    "\n",
    "# Appending tuple with POS and words\n",
    "sheet1.write('A1','Word')\n",
    "sheet1.write('B1','POS')\n",
    "add_counter1 = 0\n",
    "for sentence in taglst:\n",
    "    for tupleentry in sentence:\n",
    "        add_counter1 += 1\n",
    "        sheet1.write_row(add_counter1,0,tupleentry)\n",
    "        \n",
    "# Appending lists  \n",
    "writeCol('C1','Content/function',binary_lst)\n",
    "writeCol('D1','Set no.',setlst)\n",
    "writeCol('E1','Sentence no.',snlst)\n",
    "writeCol('F1','Passage no.',lplst_word)\n",
    "writeCol('G1','Sentence no. within passage',snplst)\n",
    "writeCol('H1','Word no. within sentence',wnslst)\n",
    "writeCol('I1','Broad topic',catlst)\n",
    "sheet1.write('J1','Specific topic')\n",
    "writeCol('K1','Word length',wordlen)\n",
    "\n",
    "# Order: 'aoa','conc','prev','ar','vale','amb','freq','sur' \n",
    "writeColList('L1','Age of acquisition',aoaMerge)\n",
    "writeColList('M1','Concreteness',concMerge)\n",
    "writeColList('N1','Prevalence',prevMerge)\n",
    "writeColList('O1','Arousal',arMerge)\n",
    "writeColList('P1','Valence',valMerge)\n",
    "writeColList('Q1','Ambiguity: all possible tags',possMerge)\n",
    "writeColList('R1','Ambiguity: percentage of dominant',ambMerge)\n",
    "writeColList('S1','Log lexical frequency',freqMerge)\n",
    "writeCol('T1','Lexical surprisal',surpVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence analyses/info graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. proportion of total words with NAs for each feature\n",
    "\n",
    "#### Contains numbers as \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoaNA = countNA(aoaMerge)\n",
    "concNA = countNA(concMerge)\n",
    "prevNA = countNA(prevMerge)\n",
    "arNA = countNA(arMerge)\n",
    "valNA = countNA(valMerge)\n",
    "ambNA = countNA(ambMerge)\n",
    "freqNA = countNA(freqMerge)\n",
    "surpNA = countNA(surpVals)\n",
    "lenNA = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot code \n",
    "\n",
    "objects = ('Age of acquisition', 'Concreteness', 'Prevalence', 'Arousal', 'Valence','Ambiguity','Frequency','Surprisal','Word Length') #'Ambiguity', 'Log lexical freq', 'Lexical surprisal')\n",
    "y_pos = np.arange(len(objects))\n",
    "counts = [aoaNA,concNA,prevNA,arNA,valNA,ambNA,freqNA,surpNA,lenNA]\n",
    "\n",
    "plt.grid(color='grey', which='both',linestyle='-', axis='y',linewidth=0.5)\n",
    "plt.bar(y_pos, counts, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('No. of NA values out of 3193 words')\n",
    "plt.title('A. Set3: No. of NA values for each feature')\n",
    "plt.savefig('A_set3.png',bbox_inches = 'tight')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "print(aoaNA,concNA,prevNA,arNA,valNA,ambNA,freqNA,surpNA,lenNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Unique no. of NAs\n",
    "#### No digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique no of NA - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueNA(tuplelst):\n",
    "    NAlst = []\n",
    "    for tup in tuplelst:\n",
    "        if tup[1] == ['#NA']:\n",
    "            NAlst.append(tup[0])\n",
    "    \n",
    "    uniqueWords = np.unique(NAlst)\n",
    "    \n",
    "    return uniqueWords\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip merged lists and wordlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(aoaNA,concNA,prevNA,arNA,valNA,ambNA,freqNA,surpNA,lenNA)\n",
    "\n",
    "aoaMergeT = list(zip(wordlst_l,aoaMerge)) #aoaMerge TUPLE\n",
    "concMergeT = list(zip(wordlst_l,concMerge)) \n",
    "prevMergeT = list(zip(wordlst_l,prevMerge)) \n",
    "arMergeT = list(zip(wordlst_l,arMerge)) \n",
    "valMergeT = list(zip(wordlst_l,valMerge)) \n",
    "ambMergeT = list(zip(wordlst_l,ambMerge)) \n",
    "freqMergeT = list(zip(wordlst_l,freqMerge)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoaU = uniqueNA(aoaMergeT)\n",
    "concU = uniqueNA(concMergeT)\n",
    "prevU = uniqueNA(prevMergeT)\n",
    "arU = uniqueNA(arMergeT)\n",
    "valU = uniqueNA(valMergeT)\n",
    "ambU = uniqueNA(ambMergeT)\n",
    "freqU = uniqueNA(freqMergeT)\n",
    "surpU = u_sur\n",
    "lenU = 0\n",
    "\n",
    "aoaUlst = aoaU.tolist()\n",
    "concUlst = concU.tolist()\n",
    "prevUlst = prevU.tolist()\n",
    "arUlst = arU.tolist()\n",
    "valUlst = valU.tolist()\n",
    "ambUlst = ambU.tolist()\n",
    "freqUlst = freqU.tolist()\n",
    "surpUlst = surpU.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Age of acquisition', 'Concreteness', 'Prevalence', 'Arousal', 'Valence','Ambiguity','Frequency','Surprisal', 'Word Length') #'Ambiguity', 'Log lexical freq', 'Lexical surprisal')\n",
    "y_pos = np.arange(len(objects))\n",
    "counts = [len(aoaU),len(concU),len(prevU),len(arU),len(valU),len(ambU),len(freqU),len(surpU),lenU]\n",
    "\n",
    "plt.grid(color='grey', which='both',linestyle='-', axis='y',linewidth=0.5)\n",
    "plt.bar(y_pos, counts, align='center',alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('No. of unique NA values out of 3193 words')\n",
    "plt.title('B. Set3: No. unique words with NA values for each feature')\n",
    "plt.savefig('B_set3.png',bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "print(len(aoaU),len(concU),len(prevU),len(arU),len(valU),len(ambU),len(freqU),len(surpU),lenU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique NA values for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_u_lst=aoaUlst+concUlst+prevUlst+arUlst+valUlst+ambUlst+freqUlst+surpUlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_lst=list(set(big_u_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_lst=sorted(u_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique NA values across all features: ' + str(len(u_lst)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. average proportion of words with NAs per sentence (mean across sentences with SEM = standard error of the mean over sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function for avg. proportion of words with NAs per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgNA(tuplelst):\n",
    "    count = 0\n",
    "    avgNA = []\n",
    "    for tup in tuplelst:\n",
    "        if tup[1].count('#NA') == 1 or tup[1].count('[#NA]') == 1:\n",
    "            count += 1\n",
    "            \n",
    "        for char in tup[0]:\n",
    "            if char != '.':\n",
    "                continue\n",
    "            if char == '.':\n",
    "                avgNA.append(count)\n",
    "                count = 0\n",
    "                \n",
    "    divLst = np.divide(avgNA,w_lst)\n",
    "    meanLst = np.mean(divLst)\n",
    "    return meanLst, divLst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(aoaNA,concNA,prevNA,arNA,valNA,ambNA,freqNA,surpNA,lenNA)\n",
    "\n",
    "aoaMergeT2 = list(zip(wordlst,aoaMerge)) #aoaMerge TUPLE\n",
    "concMergeT2 = list(zip(wordlst,concMerge)) \n",
    "prevMergeT2 = list(zip(wordlst,prevMerge)) \n",
    "arMergeT2 = list(zip(wordlst,arMerge)) \n",
    "valMergeT2 = list(zip(wordlst,valMerge)) \n",
    "ambMergeT2 = list(zip(wordlst,ambMerge)) \n",
    "freqMergeT2 = list(zip(wordlst,freqMerge)) \n",
    "surpMergeT2 = list(zip(wordlst,surpVals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoaAvgNA, aoaDiv = avgNA(aoaMergeT2)\n",
    "concAvgNA, concDiv = avgNA(concMergeT2)\n",
    "prevAvgNA, prevDiv = avgNA(prevMergeT2)\n",
    "arAvgNA, arDiv = avgNA(arMergeT2)\n",
    "valAvgNA, valDiv = avgNA(valMergeT2)\n",
    "ambAvgNA, ambDiv = avgNA(ambMergeT2)\n",
    "freqAvgNA, freqDiv = avgNA(freqMergeT2)\n",
    "surpAvgNA, surpDiv = avgNA(surpMergeT2)\n",
    "lenAvgNA = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Age of acquisition', 'Concreteness', 'Prevalence', 'Arousal', 'Valence','Ambiguity','Frequency','Surprisal', 'Word Length') #'Ambiguity', 'Log lexical freq', 'Lexical surprisal')\n",
    "y_pos = np.arange(len(objects))\n",
    "counts = [aoaAvgNA,concAvgNA,prevAvgNA,arAvgNA,valAvgNA,ambAvgNA,freqAvgNA,surpAvgNA,lenAvgNA]\n",
    "\n",
    "plt.grid(color='grey', which='both',linestyle='-', axis='y',linewidth=0.5)\n",
    "plt.bar(y_pos, counts, align='center',alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('Mean proportion of NA within each sentence')\n",
    "plt.title('C. Set3: Mean proportion of words with NA values within each sentence for each feature')\n",
    "plt.savefig('C_set3.png',bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(aoaAvgNA,concAvgNA,prevAvgNA,arAvgNA,valAvgNA,ambAvgNA,freqAvgNA,surpAvgNA,lenAvgNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Proportion of sentences where c. is 0.50 or higher "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing no of sentences where c. is >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findProp(divlst):\n",
    "    propLst = []\n",
    "    for val in divlst:\n",
    "        if val >= 0.5:\n",
    "            propLst.append(val)\n",
    "    \n",
    "    return len(propLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoaProp = findProp(aoaDiv)\n",
    "concProp = findProp(concDiv)\n",
    "prevProp = findProp(prevDiv)\n",
    "arProp = findProp(arDiv)\n",
    "valProp = findProp(valDiv)\n",
    "ambProp = findProp(ambDiv)\n",
    "freqProp = findProp(freqDiv)\n",
    "surpProp = findProp(surpDiv)\n",
    "lenProp = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Age of acquisition', 'Concreteness', 'Prevalence', 'Arousal', 'Valence','Ambiguity','Frequency','Surprisal','Word Length') #'Ambiguity', 'Log lexical freq', 'Lexical surprisal')\n",
    "y_pos = np.arange(len(objects))\n",
    "counts = [aoaProp,concProp,prevProp,arProp,valProp,ambProp,freqProp,surpProp,lenProp] \n",
    "\n",
    "plt.grid(color='grey', which='both',linestyle='-', axis='y',linewidth=0.5)\n",
    "plt.bar(y_pos, counts, align='center',alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('No. sentences out of 243 sentences')\n",
    "plt.title('D. Set3: No. sentences with mean proportion of NA values .50 or higher')\n",
    "plt.savefig('D_set3.png',bbox_inches = 'tight')\n",
    "\n",
    "plt.show()\n",
    "print(aoaProp,concProp,prevProp,arProp,valProp,ambProp,freqProp,surpProp,lenProp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging of sentence feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach: A 9 dimensional vector for each sentence is utilized.\n",
    "\n",
    "# For e.g. first sentence, first feature. 7 words. 6 values. Word with NA value omitted. The rest is used for computing avg. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tuple of all the values I want to find averages for \n",
    "\n",
    "test_tup = list(zip(aoaMerge,concMerge,prevMerge,arMerge,valMerge,ambMerge,freqMerge,surpMerge,wordlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses wordlst as the key, BUT the values correspond to the lemmatized list. Wordlst_lem DOES NOT have '.', i.e. it's harder to use.\n",
    "\n",
    "d = {} # Overall dictionary \n",
    "sentence_num = 1\n",
    "cnt = 0\n",
    "d[1] = {} # The first sentence dictionary, which contains all the words for sentence 1\n",
    "\n",
    "while cnt < len(wordlst):\n",
    "    \n",
    "    if wordlst[cnt][-1] == '.': # Checking the last char of each word\n",
    "        d[sentence_num][wordlst[cnt]] = [] # Creating a new empty dict for the given word\n",
    "        flag = False\n",
    "        for feat in test_tup[cnt]: # Iterating through each feature in the tuple list for the respective count\n",
    "            if feat == ['#NA'] or feat == '[#NA]':\n",
    "                flag = True\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                d[sentence_num][wordlst[cnt]].append(feat) # appending each feature if it is NOT a #NA\n",
    "        if flag: \n",
    "            del d[sentence_num][wordlst[cnt]]\n",
    "        sentence_num += 1\n",
    "        d[sentence_num] = {} # When every feature is appended for reach word, a new dict is created for the next sentence\n",
    "        cnt += 1\n",
    "    else:\n",
    "        d[sentence_num][wordlst[cnt]] = [] # If the character is not . appending the feature for the respective word\n",
    "        #d[sentence_num][wordlst[cnt]].append(i)\n",
    "        flag = False\n",
    "        for feat in test_tup[cnt]:\n",
    "            if feat == ['#NA'] or feat == '[#NA]':\n",
    "                flag = True\n",
    "                break\n",
    "            else:\n",
    "                d[sentence_num][wordlst[cnt]].append(feat)\n",
    "        if flag:\n",
    "            del d[sentence_num][wordlst[cnt]]\n",
    "        cnt += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in d.keys():\n",
    "    for value in d[key]:\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computing average values for each sentence, for each feature\n",
    "\n",
    "feat_names=['aoa','conc','prev','ar','val','amb','freq','surp','len']\n",
    "\n",
    "sentence_lengths = ['Sentence Lengths:']\n",
    "avg_feat_val =  {}\n",
    "for sent in d:\n",
    "    if sent == numSentences+1: \n",
    "        break\n",
    "    avg_feat_val[sent] = {}\n",
    "    for feat_name in feat_names:\n",
    "        if feat_name not in avg_feat_val[sent]:\n",
    "            avg_feat_val[sent][feat_name] = 0\n",
    "    for word in d[sent]:    \n",
    "\n",
    "        word_feats = d[sent][word]\n",
    "        i = 0\n",
    "        #print(word_feats)\n",
    "        #print(len(d[sent]))\n",
    "        for feat_name in feat_names:\n",
    "            try:\n",
    "                avg_feat_val[sent][feat_name] += word_feats[i][0] / len(d[sent])\n",
    "            except TypeError:\n",
    "                avg_feat_val[sent][feat_name] += float(word_feats[i]) / len(d[sent])\n",
    "            i += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average feature values: ' + str(avg_feat_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
